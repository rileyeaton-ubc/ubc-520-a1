\subsection{Test Data Generation}

All experiments use realistic login data generated using the Faker library \cite{faker2024}, which produces human-like usernames following common patterns:
\begin{itemize}
    \item Standard usernames (e.g., \texttt{john\_smith})
    \item First/last names with numeric suffixes (e.g., \texttt{alice123})
    \item Email prefixes (e.g., \texttt{user.name})
    \item Random alphanumeric combinations
\end{itemize}

A dataset of $\sim$6.5 million unique logins was pre-generated and stored in a text file\footnote{\url{https://github.com/rileyeaton-ubc/ubc-520-a1/blob/3bb39197d86925f1904856b6149dbae908bb190b/data/logins.txt}} to ensure consistency across all test runs. This approach eliminates variability from data generation and allows for reproducible results, while not having to waste time re-generating logins for each test.

\subsection{Test Size Selection}

The experimental design uses five test sizes: \textbf{100, 500, 1,000, 2,000, and 5,000} logins. This progression was chosen to reveal algorithmic scaling behavior across different dataset sizes.

This range was chosen to reveal scaling behavior: $n=100$ establishes baseline performance where all algorithms are reasonable, while $n=5000$ approaches practical limits for inefficient algorithms (linear search performs $\sim$12.5 million comparisons). The intermediate sizes capture the transition where algorithmic differences become measurable. Testing beyond 5,000 elements would provide diminishing returns---linear search becomes prohibitively slow, while constant-time structures show only noise rather than algorithmic differences.

\subsection{Experimental Procedure}

For each test size $n$: insert $n$ unique logins from pre-generated data, then perform $n$ lookups (50\% existing, 50\% non-existent). Two metrics are tracked: \textbf{wall-clock time} (real-world performance including overhead) and \textbf{comparison count} (implementation-independent validation of theoretical complexity). This yields 25 trials total (5 structures Ã— 5 sizes).

\subsection{Implementation Details}

All implementations use Python 3.12 with the following specifics:

\begin{itemize}
    \item \textbf{Linear Search:} Python list with sequential iteration
    \item \textbf{Binary Search:} Sorted Python list with manual binary search implementation
    \item \textbf{Hash Table:} Python's built-in \texttt{set} (hash table with open addressing)
    \item \textbf{Bloom Filter:} \texttt{pybloom-live} library with $n=1,000,000$ capacity and 0.001 error rate
    \item \textbf{Cuckoo Filter:} \texttt{cuckoo-filter} library with table size 10,000, bucket size 4, fingerprint size 8 bits
\end{itemize}

Both probabilistic filters (Bloom and Cuckoo) use a backing hash table to verify positive results, eliminating false positives at the cost of additional space overhead. This hybrid approach ensures correctness while maintaining the performance benefits of probabilistic filtering for negative lookups.

